package elasticsearch

import (
	"context"
	"fmt"
	"log"
	"sync/atomic"
	"time"

	"github.com/olivere/elastic"
	"golang.org/x/sync/errgroup"
)

var (
	// index       = "test"
	// typ         = "_doc"
	msearchSize = 10
)

// type tweet struct {
// 	Fingerprint string                `json:"fingerprint"`
// 	Message     string                `json:"message"`
// 	Retweets    int                   `json:"retweets"`
// 	Image       string                `json:"image,omitempty"`
// 	Created     time.Time             `json:"created,omitempty"`
// 	Tags        []string              `json:"tags,omitempty"`
// 	Location    string                `json:"location,omitempty"`
// 	Suggest     *elastic.SuggestField `json:"suggest_field,omitempty"`
// }

// Msearch for search and stuff
func Msearch(line string, key string, client *elastic.Client) {

	// Setup a group of goroutines from the excellent errgroup package
	g, ctx := errgroup.WithContext(context.TODO())

	// The first goroutine will emit documents and send it to the second goroutine
	// via the docsc channel.
	// The second Goroutine will simply bulk insert the documents.
	type s struct {
		Request   *elastic.SearchRequest
		Timestamp time.Time
		Hash      string
	}
	docsc := make(chan s)

	begin := time.Now()

	// Goroutine to create documents
	g.Go(func() error {
		defer close(docsc)

		// Construct the search
		matchQuery := elastic.NewMatchQuery("message", line)
		triGram := elastic.NewMatchQuery("message.tri", line)
		r := elastic.NewSearchRequest().Index(index).
			Source(elastic.NewSearchSource().Query(matchQuery).Query(triGram).From(0).Size(1))
		d := s{
			Request:   r,
			Hash:      key,
			Timestamp: time.Now(),
		}
		// Construct the document

		// Send over to 2nd goroutine, or cancel
		select {
		case docsc <- d:
		case <-ctx.Done():
			return ctx.Err()
		}
		// }
		return nil
	})

	// Second goroutine will consume the documents sent from the first and bulk insert into ES
	var total uint64

	g.Go(func() error {
		msearch := client.MultiSearch().Index(index)

		// bulk := client.Bulk().Index(index).Type(typ)
		// for d := range docsc {
		for it := range docsc {
			fmt.Println(it)
			// Simple progress
			current := atomic.AddUint64(&total, 1)
			dur := time.Since(begin).Seconds()
			sec := int(dur)
			pps := int64(float64(current) / dur)
			fmt.Printf("%10d | %6d req/s | %02d:%02d\r", current, pps, sec/60, sec%60)

			// Enqueue the document

			// bulk.Add(elastic.NewBulkIndexRequest().Doc(d))
			msearch.Add(it.Request)

			if len(docsc) >= msearchSize {

				res, err := msearch.Do(ctx)
				if err != nil {
					return err
				}
				fmt.Println(res)
			}
			// if bulk.NumberOfActions() >= bulkSize {
			// 	// Commit
			// 	res, err := bulk.Do(ctx)
			// 	if err != nil {
			// 		return err
			// 	}
			// 	if res.Errors {
			// 		// Look up the failed documents with res.Failed(), and e.g. recommit
			// 		return errors.New("bulk commit failed")
			// 	}
			// "bulk" is reset after Do, so you can reuse it
			// }

			select {
			default:
			case <-ctx.Done():
				return ctx.Err()
			}
		}

		// Commit the final batch before exiting
		// if bulk.NumberOfActions() > 0 {
		if len(docsc) > 0 {
			_, err := msearch.Do(ctx)
			if err != nil {
				return err
			}
		}
		return nil
	})

	// Wait until all goroutines are finished
	if err := g.Wait(); err != nil {
		log.Fatal(err)
	}

	// Final results
	dur := time.Since(begin).Seconds()
	sec := int(dur)
	pps := int64(float64(total) / dur)
	fmt.Printf("%10d | %6d req/s | %02d:%02d\n", total, pps, sec/60, sec%60)
}
